{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4yBrceuFbf3"
   },
   "source": [
    "#Wazy + ColabFold for Finding Peptide Binders\n",
    "\n",
    "**WARNING: This is from ongoing research that has not been peer-reviewed! Please report bugs at [Wazy on Github](https://github.com/ur-whitelab/wazy) and use caution with results from this notebook.**\n",
    "\n",
    "This notebook is used to find a peptide binder for a given protein. Affinity is assessed based on predicted distance to target binding site and AlphaFold2 confidence of peptide's structure.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "Enter your the protein sequence for what you want to bind to and optionally specify the residues you want to bind in the form below. Then go to Menu Runtime -> Run All. You can re-execute the \"Perform Optimization Cell\" additional times to increase number of optimization rounds.\n",
    "\n",
    "## Important Notes:##\n",
    "\n",
    "* The notebook by default uses faster settings so that you can explore the method/sequences. Amber Relax should be used for best accuracy & number of recycles should be 6 or greater. Click the tick box \"high quality\" to enable these settings.\n",
    "* Results will be printed, plotting, and can be downloaded by executing the prepare results button. The filenames with `bo` in them will contain the sequences and binding scores. \n",
    "\n",
    "## About Wazy\n",
    "\n",
    "Wazy is a method for optimizing sequences for a numeric task, like quantitative activity or solubility. Wazy uses Bayesian Optimization to propose which new sequences should be tried. The method is designed for when you have few (1-100) starting sequences and want to know which additional sequences to try in order to find the best. **In this notebook, wazy is being used to optimize a sequence for binding to a protein according to the AlphaFold-Multimer model. AlphaFold-Multimer is the black box model. The notebook handles testing newly proposed sequences and updating surrogate model.** See the [wazy paper](https://www.biorxiv.org/content/10.1101/2022.08.05.502972v1) and the [code](https://github.com/ur-whitelab/wazy) for complete details on how the algorithm works. This notebook was forked from [ColabFold](https://github.com/sokrypton/ColabFold). The main change was inserting Wazy code and removing custom MSA options (for simplicity).\n",
    "\n",
    "\n",
    "Credit:\n",
    "\n",
    "* [AlphaFold2 Paper](https://www.nature.com/articles/s41586-021-03819-2), [Multimer Paper](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2)\n",
    "* This doc authored by [@andrewwhite01](https://twitter.com/andrewwhite01)\n",
    "* Wazy authored by [@andrewwhite01](https://twitter.com/andrewwhite01) and [@ZiyueYang37](https://twitter.com/ZiyueYang37)\n",
    "* [Mirdita M, Schütze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all. *Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kOblAo-xetgx"
   },
   "outputs": [],
   "source": [
    "# @title Input protein sequence(s), then hit `Runtime` -> `Run all`\n",
    "from google.colab import files\n",
    "import os.path\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "from sys import version_info\n",
    "\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
    "\n",
    "\n",
    "def add_hash(x, y):\n",
    "    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n",
    "\n",
    "\n",
    "protein_sequence = \"GMTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEASAMRDQYMRTGEGFLCVFAINNTKSFEDIHQYREQIKRVKDSDDVPMVLVGNKCDLAARTVESRQAQDLARSYGIPYIETSAKTRQGVEDAFYTLVREIRQH\"  # @param {type:\"string\"}\n",
    "starting_binder = \"GGGGGGGGGGGGGGGGGGG\"  # @param {type:\"string\"}\n",
    "variable_length = False  # @param {type:\"boolean\"}\n",
    "min_peptide_length = 6  # @param {type:\"integer\"}\n",
    "# @markdown * Should the peptide binder length vary\n",
    "binding_site = \"resid 25 to 40\"  # @param {type:\"string\"}\n",
    "# @markdown * MDAnalysis selection string. Leave empty if you don't care where it binds. Examples: *resid 5 to 25*, *resid 1 to 8 or resid 3 to 10*\n",
    "high_quality = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# remove whitespaces\n",
    "protein_sequence = \"\".join(protein_sequence.split())\n",
    "\n",
    "if binding_site == \"\":\n",
    "    binding_site = \"protein\"\n",
    "\n",
    "jobname = \"wazy\"  # @param {type:\"string\"}\n",
    "# remove whitespaces\n",
    "basejobname = \"\".join(jobname.split())\n",
    "basejobname = re.sub(r\"\\W+\", \"\", basejobname)\n",
    "\n",
    "# number of models to use\n",
    "use_amber = True  # @param {type:\"boolean\"}\n",
    "template_mode = \"none\"  # @param [\"none\", \"pdb70\", \"custom\"]\n",
    "# @markdown - \"none\" = no template information is used, \"pdb70\" = detect templates in pdb70, \"custom\" - upload and search own templates (PDB or mmCIF format, see [notes below](#custom_templates))\n",
    "\n",
    "if high_quality and not use_amber:\n",
    "    print(\"Enabling AMBER relax since in high quality mode\")\n",
    "\n",
    "if template_mode == \"pdb70\":\n",
    "    use_templates = True\n",
    "    custom_template_path = None\n",
    "elif template_mode == \"custom\":\n",
    "    custom_template_path = f\"{jobname}_template\"\n",
    "    os.mkdir(custom_template_path)\n",
    "    uploaded = files.upload()\n",
    "    use_templates = True\n",
    "    for fn in uploaded.keys():\n",
    "        os.rename(fn, f\"{jobname}_template/{fn}\")\n",
    "else:\n",
    "    custom_template_path = None\n",
    "    use_templates = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C2_sh2uAonJH"
   },
   "outputs": [],
   "source": [
    "# @markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
    "msa_mode = \"mmseqs2_uniref_env\"  # @param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
    "pair_mode = (\n",
    "    \"unpaired_paired\"  # @param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
    ")\n",
    "# @markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ADDuaolKmjGW"
   },
   "outputs": [],
   "source": [
    "# @markdown ### Advanced settings\n",
    "model_type = \"AlphaFold2-multimer-v2\"\n",
    "num_models = 1  # @param {type:\"slider\", min:1, max:5, step:1}\n",
    "num_recycles = 3  # @param [1,3,6,12,24,48] {type:\"raw\"}\n",
    "save_to_google_drive = False  # @param {type:\"boolean\"}\n",
    "seed = 0  # @param {type:\"integer\"}\n",
    "\n",
    "if high_quality and num_recycles < 6:\n",
    "    print(\"increasing num_recycles since in high quality mode\")\n",
    "    num_recycles = 6\n",
    "\n",
    "# @markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
    "dpi = 200\n",
    "\n",
    "# @markdown Don't forget to hit `Runtime` -> `Run all` after updating the form.\n",
    "\n",
    "\n",
    "if save_to_google_drive:\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    print(\"You are logged into Google Drive and are good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iccGdbe_Pmt9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "%%bash -s $use_amber $use_templates $python_version\n",
    "\n",
    "set -e\n",
    "\n",
    "USE_AMBER=$1\n",
    "USE_TEMPLATES=$2\n",
    "PYTHON_VERSION=$3\n",
    "\n",
    "if [ ! -f COLABFOLD_READY ]; then\n",
    "  echo \"installing colabfold...\"\n",
    "  # install dependencies\n",
    "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
    "  pip install -q wazy MDAnalysis\n",
    "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\" \"tensorflow-cpu==2.11.0\"\n",
    "  pip uninstall -yq jax jaxlib chex optax\n",
    "  pip install -q chex==0.1.5 optax==0.1.3 \"jax[cuda]==0.3.25\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n",
    "  # for debugging\n",
    "  ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\n",
    "  ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\n",
    "  touch COLABFOLD_READY\n",
    "fi\n",
    "\n",
    "# setup conda\n",
    "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
    "  if [ ! -f CONDA_READY ]; then\n",
    "    echo \"installing conda...\"\n",
    "    wget -qnc https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh\n",
    "    bash Mambaforge-colab-Linux-x86_64.sh -bfp /usr/local\n",
    "    mamba config --set auto_update_conda false\n",
    "    touch CONDA_READY\n",
    "  fi\n",
    "fi\n",
    "# setup template search\n",
    "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ] && [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
    "  echo \"installing hhsuite...\"\n",
    "  mamba install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python=\"${PYTHON_VERSION}\" pdbfixer 2>&1 1>/dev/null\n",
    "  touch HH_READY\n",
    "  touch AMBER_READY\n",
    "fi\n",
    "# setup openmm for amber refinement\n",
    "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
    "  mamba install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=\"${PYTHON_VERSION}\" 2>&1 1>/dev/null\n",
    "fi\n",
    "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
    "  echo \"installing amber...\"\n",
    "  mamba install -y -q -c conda-forge openmm=7.7.0 python=\"${PYTHON_VERSION}\" pdbfixer 2>&1 1>/dev/null\n",
    "  touch AMBER_READY\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_sztQyz29DIC"
   },
   "outputs": [],
   "source": [
    "# @title Set-up AlphaFold\n",
    "# @markdown **WARNING: Running this a second time will remove all cached results!**\n",
    "\n",
    "import sys, json, glob\n",
    "import urllib.request\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from Bio import BiopythonDeprecationWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=BiopythonDeprecationWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "\n",
    "try:\n",
    "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "except:\n",
    "    K80_chk = \"0\"\n",
    "    pass\n",
    "if \"1\" in K80_chk:\n",
    "    print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "    if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "        del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "    if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "        del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "\n",
    "from colabfold.colabfold import plot_protein\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import MDAnalysis as mda\n",
    "import MDAnalysis.analysis.distances as mda_dist\n",
    "import jax\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/courierprime/CourierPrime-Regular.ttf\",\n",
    "    \"CourierPrime-Regular.ttf\",\n",
    ")\n",
    "fe = mpl.font_manager.FontEntry(fname=\"CourierPrime-Regular.ttf\", name=\"courierprime\")\n",
    "mpl.font_manager.fontManager.ttflist.append(fe)\n",
    "color_cycle = [\"#444444\", \"#1BBC9B\", \"#a895bb\", \"#F06060\", \"#F3B562\", \"#80cedb\"]\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(color=color_cycle),\n",
    "        \"font.family\": fe.name,\n",
    "        \"font.size\": 14,\n",
    "        \"figure.figsize\": (4.5, 4.5 / 1.3),\n",
    "        \"figure.dpi\": 100,\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "        \"image.cmap\": \"gist_yarg\",\n",
    "        \"lines.markersize\": 6,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "# For some reason we need that to get pdbfixer to import\n",
    "if (\n",
    "    use_amber\n",
    "    and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path\n",
    "):\n",
    "    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
    "\n",
    "result_dir = \".\"\n",
    "if \"logging_setup\" not in globals():\n",
    "    setup_logging(Path(\".\").joinpath(\"log.txt\"))\n",
    "    logging_setup = True\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def label_sequence(seq, model_type=model_type, af_model=1):\n",
    "    query_sequence = protein_sequence + \":\" + seq\n",
    "    jobname = add_hash(basejobname, query_sequence)\n",
    "    print(jobname)\n",
    "    while os.path.isfile(f\"{jobname}.csv\"):\n",
    "        jobname = add_hash(\n",
    "            basejobname, \"\".join(random.sample(query_sequence, len(query_sequence)))\n",
    "        )\n",
    "\n",
    "    with open(f\"{jobname}.csv\", \"w\") as text_file:\n",
    "        text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
    "\n",
    "    queries_path = f\"{jobname}.csv\"\n",
    "\n",
    "    # decide which a3m to use\n",
    "\n",
    "    if \"mmseqs2\" in msa_mode:\n",
    "        a3m_file = os.path.join(jobname, f\"{jobname}.a3m\")\n",
    "\n",
    "    elif msa_mode == \"custom\":\n",
    "        a3m_file = os.path.join(jobname, f\"{jobname}.custom.a3m\")\n",
    "        if not os.path.isfile(a3m_file):\n",
    "            custom_msa_dict = files.upload()\n",
    "            custom_msa = list(custom_msa_dict.keys())[0]\n",
    "            header = 0\n",
    "            import fileinput\n",
    "\n",
    "            for line in fileinput.FileInput(custom_msa, inplace=1):\n",
    "                if line.startswith(\">\"):\n",
    "                    header = header + 1\n",
    "                if not line.rstrip():\n",
    "                    continue\n",
    "                if line.startswith(\">\") == False and header == 1:\n",
    "                    query_sequence = line.rstrip()\n",
    "                print(line, end=\"\")\n",
    "\n",
    "            os.rename(custom_msa, a3m_file)\n",
    "            queries_path = a3m_file\n",
    "            print(f\"moving {custom_msa} to {a3m_file}\")\n",
    "\n",
    "    else:\n",
    "        a3m_file = os.path.join(jobname, f\"{jobname}.single_sequence.a3m\")\n",
    "        with open(a3m_file, \"w\") as text_file:\n",
    "            text_file.write(\">1\\n%s\" % query_sequence)\n",
    "\n",
    "    queries, is_complex = get_queries(queries_path)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "    download_alphafold_params(model_type, Path(\".\"))\n",
    "    run(\n",
    "        queries=queries,\n",
    "        result_dir=result_dir,\n",
    "        use_templates=use_templates,\n",
    "        custom_template_path=custom_template_path,\n",
    "        use_amber=use_amber,\n",
    "        msa_mode=msa_mode,\n",
    "        model_type=model_type,\n",
    "        num_models=1,\n",
    "        num_recycles=num_recycles,\n",
    "        model_order=[af_model],\n",
    "        is_complex=is_complex,\n",
    "        data_dir=Path(\".\"),\n",
    "        keep_existing_results=False,\n",
    "        recompile_padding=1.0,\n",
    "        rank_by=\"auto\",\n",
    "        pair_mode=pair_mode,\n",
    "        stop_at_score=float(100),\n",
    "    )\n",
    "    rank_num = 1\n",
    "    try:\n",
    "        glob_plddt = f\"{jobname}_scores_rank_*_alphafold2_multimer_v2_model*.json\"\n",
    "        plddt_paths = glob.glob(glob_plddt)[0]\n",
    "        os.system(f\"cp {plddt_paths} plddt.json\")\n",
    "        f = open(\"plddt.json\")\n",
    "    except FileNotFoundError as e:\n",
    "        glob_plddt = f\"{jobname}_scores_rank_*_alphafold2_multimer_v2_model*.json\"\n",
    "        plddt_paths = glob.glob(glob_plddt)[0]\n",
    "        os.system(f\"cp {plddt_paths} plddt.json\")\n",
    "        f = open(\"plddt.json\")\n",
    "\n",
    "    data = json.load(f)\n",
    "    avg_plddt = np.mean(data[\"plddt\"][len(protein_sequence) :])\n",
    "\n",
    "    pdb_filename = f\"{jobname}_{'' if use_amber else 'un'}relaxed_rank_*.pdb\"\n",
    "    pdb_file = glob.glob(pdb_filename)[0]\n",
    "    # load and find smallest chain\n",
    "    u = mda.Universe(pdb_file)\n",
    "    peptide = None\n",
    "    for chain in u.segments:\n",
    "        if peptide is None or len(chain.residues) < len(peptide):\n",
    "            peptide = chain.residues\n",
    "    protein = u.select_atoms(\n",
    "        f\"({binding_site}) and not segid {peptide.segids[0]} and not name H*\"\n",
    "    )\n",
    "    peptide = peptide.atoms.select_atoms(\"not name H*\")\n",
    "    all_d = []\n",
    "    for r in peptide.residues:\n",
    "        distances = mda_dist.distance_array(r.atoms.positions, protein.positions)\n",
    "        # get row, column of minimum distance\n",
    "        i, j = np.unravel_index(distances.argmin(), distances.shape)\n",
    "        all_d.append(distances[i, j])\n",
    "    avg_d = np.mean(all_d)\n",
    "\n",
    "    score = avg_plddt / 50 - avg_d / 10\n",
    "    return {\n",
    "        \"seq\": seq,\n",
    "        \"score\": score,\n",
    "        \"rmsd\": avg_d,\n",
    "        \"plddt\": avg_plddt,\n",
    "        \"id\": jobname,\n",
    "    }\n",
    "\n",
    "\n",
    "# will be cached, so we aren't wasting time here\n",
    "_ = label_sequence(starting_binder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FRp599rcBhb-"
   },
   "outputs": [],
   "source": [
    "# @title Perform optimization. Re-execute as many times as desired.\n",
    "import wazy\n",
    "\n",
    "boa = wazy.BOAlgorithm()\n",
    "bo_results = []\n",
    "\n",
    "iterations = 10  # @param {type:\"integer\"}\n",
    "\n",
    "seq = starting_binder\n",
    "for i in range(iterations):\n",
    "    key, _ = jax.random.split(key)\n",
    "    y = label_sequence(seq)\n",
    "    bo_results.append(y)\n",
    "    boa.tell(key, seq, label=y[\"score\"])\n",
    "    print(f'Tell#{i} {seq} score={y[\"score\"]} rmsd={y[\"rmsd\"]} plddt={y[\"plddt\"]}')\n",
    "    if i < iterations - 1:\n",
    "        aq = \"ucb\"\n",
    "    else:\n",
    "        aq = \"max\"\n",
    "    if variable_length:\n",
    "        score = None\n",
    "        for l in range(len(seq) - 1, len(seq) + 2):\n",
    "            if l < min_peptide_length:\n",
    "                continue\n",
    "            seq_l, score_l = boa.ask(key, aq_fxn=aq, length=l)\n",
    "            if score is None or score_l > score:\n",
    "                seq = seq_l\n",
    "                score = score_l\n",
    "    else:\n",
    "        seq, score = boa.ask(key, aq_fxn=aq)\n",
    "    print(f\"Ask#{i} L={len(seq)}  {seq} {score[0]}\")\n",
    "\n",
    "y = label_sequence(seq)\n",
    "print(f'Tell#{i} {seq} score={y[\"score\"]} rmsd={y[\"rmsd\"]} plddt={y[\"plddt\"]}')\n",
    "bo_results.append(y)\n",
    "boa.tell(key, seq, label=y[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "11l8k--10q0C"
   },
   "outputs": [],
   "source": [
    "# @title Plot Results\n",
    "\n",
    "x = np.arange(len(bo_results))\n",
    "y1 = [r[\"rmsd\"] for r in bo_results]\n",
    "y2 = [r[\"plddt\"] for r in bo_results]\n",
    "y3 = [r[\"score\"] for r in bo_results]\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(x, y1, label=\"RMSD\")\n",
    "ax1.set_ylabel(\"RSMD\")\n",
    "ax2 = plt.twinx()\n",
    "ax2.plot(x, y2, color=\"C1\", label=\"pLDDT\")\n",
    "ax2.set_ylabel(\"pLDDT\")\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=\"upper center\")\n",
    "plt.savefig(f\"{basejobname}_bo.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y3, label=\"Score\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.savefig(f\"{basejobname}_bo_score.png\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "with open(f\"{basejobname}_bo.json\", \"w\") as f, open(f\"{basejobname}_bo.txt\", \"w\") as g:\n",
    "    out = []\n",
    "    for i, seq in enumerate(boa.seqs):\n",
    "        # exploiting cache\n",
    "        r = label_sequence(seq)\n",
    "        out.append({\"step\": i, **r})\n",
    "        if i == 0:\n",
    "            g.write(\"step,sequence\" + \",\".join(r.keys()) + \"\\n\")\n",
    "        g.write(f\"{i},{seq}\" + \",\".join([str(v) for v in r.values()]) + \"\\n\")\n",
    "        print(f'#{i:2d}. {seq:20} {r[\"rmsd\"]:2.1f} {r[\"plddt\"]:3.1f} {r[\"score\"]:1.3f}')\n",
    "    json.dump(out, f, indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "33g5IIegij5R"
   },
   "outputs": [],
   "source": [
    "# @title Package and download results\n",
    "# @markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
    "\n",
    "\n",
    "import locale\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "if msa_mode == \"custom\":\n",
    "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
    "\n",
    "!zip -FSr $jobname\".result.zip\" config.json $jobname*\".json\" $jobname*\".a3m\" $jobname*\"relaxed_rank_\"*\".pdb\" \"cite.bibtex\" $jobname*\".png\" $basejobname\"_bo\"*\n",
    "files.download(f\"{jobname}.result.zip\")\n",
    "\n",
    "if save_to_google_drive == True and drive:\n",
    "    uploaded = drive.CreateFile({\"title\": f\"{jobname}.result.zip\"})\n",
    "    uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
    "    uploaded.Upload()\n",
    "    print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW4MBfQa8m0b"
   },
   "source": [
    "## Deatiled AlphaFold Instructions <a name=\"Instructions\"></a>\n",
    "**Quick start**\n",
    "1. Paste your protein sequence(s) in the input field.\n",
    "2. Press \"Runtime\" -> \"Run all\".\n",
    "3. The pipeline consists of 5 steps. The currently running step is indicated by a circle with a stop sign next to it.\n",
    "\n",
    "**Result zip file contents**\n",
    "\n",
    "1. PDB formatted structures sorted by avg. pLDDT and complexes are sorted by pTMscore. (unrelaxed and relaxed if `use_amber` is enabled).\n",
    "2. Plots of the model quality.\n",
    "3. Plots of the MSA coverage.\n",
    "4. Parameter log file.\n",
    "5. A3M formatted input MSA.\n",
    "6. A `predicted_aligned_error_v1.json` using [AlphaFold-DB's format](https://alphafold.ebi.ac.uk/faq#faq-7) and a `scores.json` for each model which contains an array (list of lists) for PAE, a list with the average pLDDT and the pTMscore.\n",
    "7. BibTeX file with citations for all used tools and databases.\n",
    "\n",
    "At the end of the job a download modal box will pop up with a `jobname.result.zip` file. Additionally, if the `save_to_google_drive` option was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
    "\n",
    "**MSA generation for complexes**\n",
    "\n",
    "For the complex prediction we use unpaired and paired MSAs. Unpaired MSA is generated the same way as for the protein structures prediction by searching the UniRef100 and environmental sequences three iterations each.\n",
    "\n",
    "The paired MSA is generated by searching the UniRef100 database and pairing the best hits sharing the same NCBI taxonomic identifier (=species or sub-species). We only pair sequences if all of the query sequences are present for the respective taxonomic identifier.\n",
    "\n",
    "**Using a custom MSA as input**\n",
    "\n",
    "To predict the structure with a custom MSA (A3M formatted): (1) Change the `msa_mode`: to \"custom\", (2) Wait for an upload box to appear at the end of the \"MSA options ...\" box. Upload your A3M. The first fasta entry of the A3M must be the query sequence without gaps. \n",
    "\n",
    "It is also possilbe to proide custom MSAs for complex predictions. Read more about the format [here](https://github.com/sokrypton/ColabFold/issues/76).\n",
    "\n",
    "As an alternative for MSA generation the [HHblits Toolkit server](https://toolkit.tuebingen.mpg.de/tools/hhblits) can be used. After submitting your query, click \"Query Template MSA\" -> \"Download Full A3M\". Download the A3M file and upload it in this notebook.\n",
    "\n",
    "**Using custom templates** <a name=\"custom_templates\"></a>\n",
    "\n",
    "To predict the structure with a custom template (PDB or mmCIF formatted): (1) change the `template_mode` to \"custom\" in the execute cell and (2) wait for an upload box to appear at the end of the \"Input Protein\" box. Select and upload your templates (multiple choices are possible).\n",
    "\n",
    "* Templates must follow the four letter PDB naming.\n",
    "\n",
    "* Templates in mmCIF format must contain `_entity_poly_seq`. An error is thrown if this field is not present. The field `_pdbx_audit_revision_history.revision_date` is automatically generated if it is not present.\n",
    "\n",
    "* Templates in PDB format are automatically converted to the mmCIF format. `_entity_poly_seq` and `_pdbx_audit_revision_history.revision_date` are automatically generated.\n",
    "\n",
    "If you encounter problems, please report them to this [issue](https://github.com/sokrypton/ColabFold/issues/177).\n",
    "\n",
    "**Comparison to the full AlphaFold2 and Alphafold2 colab**\n",
    "\n",
    "This notebook replaces the homology detection and MSA pairing of AlphaFold2 with MMseqs2. For a comparison against the [AlphaFold2 Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) and the full [AlphaFold2](https://github.com/deepmind/alphafold) system read our [preprint](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1). \n",
    "\n",
    "**Troubleshooting**\n",
    "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
    "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
    "* Check your input sequence.\n",
    "\n",
    "**Known issues**\n",
    "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
    "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
    "\n",
    "**Limitations**\n",
    "* Computing resources: Our MMseqs2 API can handle ~20-50k requests per day.\n",
    "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or MGnify.\n",
    "* We recommend to additionally use the full [AlphaFold2 pipeline](https://github.com/deepmind/alphafold).\n",
    "\n",
    "**Description of the plots**\n",
    "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
    "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
    "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
    "\n",
    "**Bugs**\n",
    "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
    "\n",
    "**License**\n",
    "\n",
    "The source code of ColabFold is licensed under [MIT](https://raw.githubusercontent.com/sokrypton/ColabFold/main/LICENSE). Additionally, this notebook uses the AlphaFold2 source code and its parameters licensed under [Apache 2.0](https://raw.githubusercontent.com/deepmind/alphafold/main/LICENSE) and [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively. Read more about the AlphaFold license [here](https://github.com/deepmind/alphafold).\n",
    "\n",
    "**Acknowledgments**\n",
    "- We thank the AlphaFold team for developing an excellent model and open sourcing the software. \n",
    "\n",
    "- [KOBIC](https://kobic.re.kr) and [Söding Lab](https://www.mpinat.mpg.de/soeding) for providing the computational resources for the MMseqs2 MSA server.\n",
    "\n",
    "- Richard Evans for helping to benchmark the ColabFold's Alphafold-multimer support.\n",
    "\n",
    "- [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin, without whom these notebooks would be quite boring!\n",
    "\n",
    "- Do-Yoon Kim for creating the ColabFold logo.\n",
    "\n",
    "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
