{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wazy.utils import *\n",
    "from wazy.mlp import *\n",
    "from jax_unirep import get_reps\n",
    "import wazy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax_unirep\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_list = [\n",
    "    \"A\",\n",
    "    \"R\",\n",
    "    \"N\",\n",
    "    \"D\",\n",
    "    \"C\",\n",
    "    \"Q\",\n",
    "    \"E\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"L\",\n",
    "    \"K\",\n",
    "    \"M\",\n",
    "    \"F\",\n",
    "    \"P\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"V\",\n",
    "    \"B\",\n",
    "    \"Z\",\n",
    "    \"X\",\n",
    "    \"*\",\n",
    "]\n",
    "blosum92 = np.loadtxt(\"./blosum62.txt\", dtype=\"i\", delimiter=\" \")\n",
    "avg92 = jnp.sum(blosum92) / 24 / 24\n",
    "sum92 = 0.0\n",
    "for row in blosum92:\n",
    "    for aa in row:\n",
    "        sum92 += (aa - avg92) ** 2\n",
    "std92 = jnp.sqrt(sum92 / 24 / 24)\n",
    "\n",
    "\n",
    "def blosum(seq1, seq2):\n",
    "    seqlist1 = list(seq1)\n",
    "    seqlist2 = list(seq2)\n",
    "    score = 0.0\n",
    "    for i in range(len(seqlist1)):\n",
    "        idx1 = AA_list.index(seqlist1[i])\n",
    "        idx2 = AA_list.index(seqlist2[i])\n",
    "        score += (blosum92[idx1][idx2] - avg92) / std92\n",
    "        # jax.nn.sigmoid(score/len(seqlist1))\n",
    "    return score / len(seqlist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = \"TARGETPEPTIDE\"\n",
    "oh_vec = encode_seq(list(target_seq))\n",
    "oh_unirep = seq2useq(oh_vec)\n",
    "target_rep = differentiable_jax_unirep(oh_unirep)\n",
    "seqs = [\"ISQLRYICEVIWF\"]\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "target_seq = 'GIGAVLKVLT'\n",
    "oh_vec = encode_seq(list(target_seq))\n",
    "oh_unirep = seq2useq(oh_vec)\n",
    "target_rep = differentiable_jax_unirep(oh_unirep)\n",
    "\n",
    "seqs = ['GIGAVLKVLKAGLPALIVTLKRKIVQ',\n",
    "       'PPGATLKKHTTGSVALISWIWARIQQ',\n",
    "       'GIGAVLKVLTTGLKTLISAAKRKRAA',\n",
    "       'HAPPVLKVLTTGLAPPLVWIKRKRTH',\n",
    "       'GIGAVLUIHKLSSVAAWRPPKRKRQQ',\n",
    "       'PTWIIFLKAQWEQHSNLTNMRTFPEV',\n",
    "        'TISHFVCNHDICAWIKDMQAMQIKMC',\n",
    "        'CESWLWKRLFDGHADRWRSMPDYPIW',\n",
    "        'YLVENPLMFPLVAAFIHQWTRQISWH',\n",
    "        'QTEERLEAQISIYYIGAWSHYKVTDE']\n",
    "\n",
    "\n",
    "seqs = ['SSVAAWRPPK',\n",
    "       'PPGATLKKHT',\n",
    "       'LTGAVLKVLK',\n",
    "       'HAPPVLKVLT',\n",
    "       'AWRPPKRKRQ',\n",
    "       'PTWIIFLKAQ',\n",
    "       'TISHFVCNHD',\n",
    "       'CESWLWKRLF',\n",
    "       'YLVENPLMFP',\n",
    "       'QTEERLEAQI',\n",
    "       'GAWSHYKVTD',\n",
    "       'HADRWRSMPD']\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "class NaiveBlock(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = hk.nets.MLP(\n",
    "            (\n",
    "                256,\n",
    "                128,\n",
    "                64,\n",
    "                1,\n",
    "            )\n",
    "        )(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def naive_forward(x):\n",
    "    f = NaiveBlock()\n",
    "    return f(x)\n",
    "\n",
    "\n",
    "naive_forward_t = hk.without_apply_rng(hk.transform(naive_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naive_loss(forward, params, seq, label):\n",
    "    yhat = forward(params, seq)  # scalar\n",
    "    return (label - yhat) ** 2\n",
    "\n",
    "\n",
    "def _shuffle_in_unison(key, a, b):\n",
    "    # NOTE to future self: do not try to rely on keys being same\n",
    "    # something about shape of arrays makes shuffle not the same\n",
    "    assert len(a) == len(b)\n",
    "    p = jax.random.permutation(key, len(a))\n",
    "    return jnp.array([a[i] for i in p]), jnp.array([b[i] for i in p])\n",
    "\n",
    "\n",
    "def _fill_to_batch(x, y, key, batch_size):\n",
    "    if len(y) >= batch_size:\n",
    "        return x, y\n",
    "    i = jax.random.choice(key, jnp.arange(len(y)), shape=(batch_size,), replace=True)\n",
    "    x = x[i, ...]\n",
    "    y = y[i, ...]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def naive_train(\n",
    "    key,\n",
    "    forward_t,\n",
    "    seqs,\n",
    "    labels,\n",
    "    val_seqs=None,\n",
    "    val_labels=None,\n",
    "    params=None,\n",
    "    epochs=3,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-2,\n",
    "):\n",
    "    opt_init, opt_update = optax.chain(\n",
    "        optax.scale_by_adam(b1=0.8, b2=0.9, eps=1e-4),\n",
    "        optax.scale(-learning_rate),  # minus sign -- minimizing the loss\n",
    "    )\n",
    "\n",
    "    key, bkey = jax.random.split(key)\n",
    "\n",
    "    # fill in seqs/labels if too small\n",
    "    seqs, labels = _fill_to_batch(seqs, labels, bkey, batch_size)\n",
    "\n",
    "    if params == None:\n",
    "        params = forward_t.init(key, seqs[0])\n",
    "\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    # wrap loss in batch/sum\n",
    "    loss_ = partial(_naive_loss, forward_t.apply)\n",
    "    loss_fxn = lambda *args: jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0))(*args))\n",
    "    # loss_fxn = jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0)))\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(opt_state, params, seqs, labels):\n",
    "        loss, grad = jax.value_and_grad(loss_fxn, 0)(params, seqs, labels)\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return opt_state, params, loss\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle seqs and labels\n",
    "        key, key_ = jax.random.split(key, num=2)\n",
    "        shuffle_seqs, shuffle_labels = _shuffle_in_unison(key, seqs, labels)\n",
    "        for i in range(0, len(shuffle_labels) // batch_size):\n",
    "            seq = shuffle_seqs[i : (i + 1) * batch_size]\n",
    "            label = shuffle_labels[i : (i + 1) * batch_size]\n",
    "            opt_state, params, loss = train_step(opt_state, params, seq, label)\n",
    "            losses.append(loss)\n",
    "        # compute validation loss\n",
    "        if val_seqs is not None:\n",
    "            val_loss = 0.0\n",
    "            for i in range(0, len(val_labels) // batch_size):\n",
    "                seq = shuffle_seqs[i : (i + 1) * batch_size]\n",
    "                label = shuffle_seqs[i : (i + 1) * batch_size]\n",
    "                val_loss += loss_fxn(\n",
    "                    params,\n",
    "                    val_seqs[i : (i + 1) * batch_size],\n",
    "                    val_labels[i : (i + 1) * batch_size],\n",
    "                )\n",
    "            val_loss = val_loss / len(val_labels) * batch_size\n",
    "            # batch_loss += loss\n",
    "            val_losses.append(val_loss)\n",
    "    return (params, losses) if val_seqs is None else (params, losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, losses = naive_train(\n",
    "    key, naive_forward_t, reps, labels, epochs=160, learning_rate=1e-3\n",
    ")\n",
    "# forward_t = hk.without_apply_rng(hk.transform(forward_fxn))\n",
    "# forward = functools.partial(forward_t.apply, params)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(params, logits):  # params is trained mlp params\n",
    "    s = wazy.SeqpropBlock()(logits)\n",
    "    us = wazy.seq2useq(s)\n",
    "    u = wazy.differentiable_jax_unirep(us)\n",
    "    forward = functools.partial(naive_forward_t.apply, params)\n",
    "    return forward(u)\n",
    "\n",
    "\n",
    "# e2e_t = hk.transform(e2e)\n",
    "# init_logits = jax.random.normal(key, shape=((10, 20)))\n",
    "# e2e_params = e2e_t.init(key, init_logits)\n",
    "\n",
    "\n",
    "def e2e_fxn(e2e_t, x, key):\n",
    "    e2e_params, logits = x\n",
    "    yhat = e2e_t.apply(e2e_params, key, logits)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_opt(key, f, init_x=None, iter_num=500, learning_rate=1e-2):\n",
    "    optimizer = optax.adam(-learning_rate)\n",
    "    opt_state = optimizer.init(init_x)\n",
    "    x = init_x\n",
    "    reduced_f = lambda *args: jnp.mean(f(*args))\n",
    "\n",
    "    @jax.jit\n",
    "    def step(x, opt_state, key):\n",
    "        loss, g = jax.value_and_grad(reduced_f, 0)(x, key)\n",
    "        updates, opt_state = optimizer.update(g, opt_state)\n",
    "        x = optax.apply_updates(x, updates)\n",
    "        return x, opt_state, loss\n",
    "\n",
    "    losses = []\n",
    "    for step_idx in range(iter_num):\n",
    "        key, _ = jax.random.split(key, num=2)\n",
    "        x, opt_state, loss = step(x, opt_state, key)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return x, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "BO_batch_size = 16\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "def loop(key, reps, labels, params, rb=None):\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    params, mlp_loss = naive_train(\n",
    "        key, naive_forward_t, reps, labels, params=params, epochs=160\n",
    "    )\n",
    "    # make random point\n",
    "    init_logits = 0.1 * jax.random.normal(key, shape=((13, 20)))\n",
    "    e2e_ = lambda logits: functools.partial(e2e, params)(logits)\n",
    "    e2e_t = hk.transform(e2e_)\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    if rb is None:\n",
    "        rb = e2e_t.init(key, init_logits)\n",
    "    e2e_params = rb\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    # init_x = 0.1*jax.random.normal(key, shape=(BO_batch_size, 10, 20))\n",
    "    # forward = jax.vmap(lambda x, key: forward_t.apply(params, x), in_axes=(0, None))\n",
    "    print(\"start optimizing\")\n",
    "    # batch_e2e = jax.vmap(lambda x, key: functools.partial(e2e_fxn, e2e_t)(params, x), in_axes=((None, 0), None))\n",
    "    batch_e2e = functools.partial(e2e_fxn, e2e_t)\n",
    "    # batched_x, bo_losses = wazy.mlp.bayes_opt(key, batch_e2e, labels, (e2e_t.init(train_key, init_logits), init_x), epsilon=0.01, iter_num=500)\n",
    "    x, losses = gradient_opt(key, batch_e2e, (e2e_params, init_logits))\n",
    "    # top_idx = np.argmin(bo_losses[-1])\n",
    "    rb = x[0]\n",
    "    logits = x[1]\n",
    "\n",
    "    vec = wazy.seq.forward_seqprop.apply(rb, key, logits)\n",
    "    s = decode_seq(vec)\n",
    "    reps = np.concatenate((reps, get_reps([s])[0]))\n",
    "    # print(get_reps([s])[0])\n",
    "    y = blosum(target_seq, s)\n",
    "    predicted_y = naive_forward_t.apply(params, get_reps([s])[0])\n",
    "    print(reps.shape)\n",
    "    print(s, y, predicted_y)\n",
    "    labels = np.concatenate(\n",
    "        (\n",
    "            labels,\n",
    "            np.array(y).reshape(\n",
    "                1,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    return key, reps, labels, s, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "y = []\n",
    "yhat = []\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    rb = None\n",
    "    params = None\n",
    "    key, reps, labels, final_vec, params = loop(key, reps, labels, params)\n",
    "    print(final_vec)\n",
    "    y.append(blosum(target_seq, final_vec))\n",
    "    yhat.append(naive_forward_t.apply(params, get_reps([final_vec])[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y, label=\"y\")\n",
    "plt.plot(yhat, label=\"yhat\")\n",
    "plt.legend()\n",
    "plt.title(\"baseline(max)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (alp)",
   "language": "python",
   "name": "alp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
